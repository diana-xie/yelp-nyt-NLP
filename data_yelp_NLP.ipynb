{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl \n",
    "%matplotlib inline\n",
    "mpl.rcParams['patch.force_edgecolor'] = True\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nyt_yelp = pd.read_pickle('import/df_nyt_yelp.pkl') # import nyt-yelp master dataframe\n",
    "df_reviews = pd.read_pickle('import/df_reviews.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Yelp reviews - NLP corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 NLP pre-processing\n",
    "\n",
    "Generate a pre-processed, tokenized list of documents in preparation for using gensim to create a corpus. Here, a <u>document</u> = a review's text. Each document is converted to a list of pre-processed tokens (not unique - will list all instances of tokens).\n",
    "\n",
    "Pre-processing steps: \n",
    "- Lowercase\n",
    "- Remove non-alphabetic characters/punctuation\n",
    "- Remove stop words\n",
    "- Lemmatize\n",
    "- Correct (some) misspellings w/ [TextBlob](http://textblob.readthedocs.io/en/dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "#from textblob import TextBlob # when this is commented out, we are running w/o TextBlob corrections, because it adds ~20 hrs\n",
    "\n",
    "text_tokenized = []\n",
    "text_preprocessed = []\n",
    "\n",
    "for idx, review in df_reviews.iterrows():\n",
    "    \n",
    "    # basic pre-processing\n",
    "    text = review['review_text'].lower() # lowercase doc\n",
    "    #text = str(TextBlob(text).correct())\n",
    "    \n",
    "    text2 = word_tokenize(text) # tokenize doc\n",
    "    text3 = [tok for tok in text2 if tok.isalpha()] # retain only alphabetic words\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english')) # generate stopwords from English dictionary\n",
    "    text4 = [tok for tok in text3 if tok not in stop_words]\n",
    "    \n",
    "    # lemmatize tokens\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text5 = [wordnet_lemmatizer.lemmatize(tok) for tok in text4]\n",
    "    \n",
    "    text6 = ' '.join(text5)\n",
    "    # correct some misspellings\n",
    "    #text6 = [str(TextBlob(tok).correct()) for tok in text5]\n",
    "    \n",
    "    text_tokenized.append(text5)\n",
    "    text_preprocessed.append(text6)\n",
    "\n",
    "df_reviews['text_preprocessed'] = text_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Create corpus w/ gensim\n",
    "\n",
    "We use [gensim](https://radimrehurek.com/gensim/) to create a corpus, where each token is mapped to a unique numerical ID and word count (i.e. bag of words, BoW) in order to set up structure for inputting to NLP algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\diana\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# create dictionary from list of pre-processed tokens (all instances) across all documents ('lemmatized')\n",
    "dictionary = Dictionary(text_tokenized)\n",
    "\n",
    "# generate corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in text_tokenized] # .doc2bow method converts documents into BoW format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a sample review under our different processing steps leading up to gensim corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review (after pre-processing):  i don't give 5 stars often, unless it was truly a stellar meal. and of course it doesn't have to be a stuffy fancypants place to be stellar, although sometimes it is. shuko was one of those upscale places and was truly, from the bottom of my heart, one of the best meals i've ever had. it's so badass that the door isn't even marked- it's this elite club that you enter because you know about it already. i got the omakase menu, every bite was carefully prepared in front of my eyes before being placed on my little stone tray and then popped into my mouth. it was as if centuries of preparation and thought had gone into the making of each bite; the culture and technique behind the assembly of the sushi, the flavor and texture profiles, the quality of the ingredients... i think when i went through that unmarked black door, i went to narnia and then reemerged into the mundane world upon exit. sake was phenomenal. i tasted everything that the sommelier (sakelier? sake master?) was describing. service was awesome and very pleasant, we had a blast with the bartender. if you can get a res and have the cash to finance, go to shuko. if not, picture how the gods eat and you got yourself a fair approximation of this meal \n",
      "\n",
      "Review (after document tokenization, removing stopwords, lemmatization):  ['give', 'star', 'often', 'unless', 'truly', 'stellar', 'meal', 'course', 'stuffy', 'fancypants', 'place', 'stellar', 'although', 'sometimes', 'shuko', 'one', 'upscale', 'place', 'truly', 'bottom', 'heart', 'one', 'best', 'meal', 'ever', 'badass', 'door', 'even', 'elite', 'club', 'enter', 'know', 'already', 'got', 'omakase', 'menu', 'every', 'bite', 'carefully', 'prepared', 'front', 'eye', 'placed', 'little', 'stone', 'tray', 'popped', 'mouth', 'century', 'preparation', 'thought', 'gone', 'making', 'bite', 'culture', 'technique', 'behind', 'assembly', 'sushi', 'flavor', 'texture', 'profile', 'quality', 'ingredient', 'think', 'went', 'unmarked', 'black', 'door', 'went', 'narnia', 'reemerged', 'mundane', 'world', 'upon', 'exit', 'sake', 'phenomenal', 'tasted', 'everything', 'sommelier', 'sakelier', 'sake', 'master', 'describing', 'service', 'awesome', 'pleasant', 'blast', 'bartender', 'get', 're', 'cash', 'finance', 'go', 'shuko', 'picture', 'god', 'eat', 'got', 'fair', 'approximation', 'meal'] \n",
      "\n",
      "Review (after gensim corpus):  [(34, 1), (40, 1), (44, 2), (48, 1), (60, 1), (85, 2), (108, 1), (123, 2), (130, 1), (157, 1), (184, 1), (191, 1), (193, 1), (194, 1), (201, 3), (210, 2), (229, 1), (267, 1), (282, 2), (287, 1), (300, 1), (347, 1), (363, 1), (374, 1), (570, 1), (606, 1), (613, 1), (652, 1), (657, 1), (679, 1), (720, 1), (737, 1), (752, 1), (784, 1), (848, 1), (867, 1), (949, 1), (978, 1), (1027, 1), (1063, 1), (1126, 2), (1132, 1), (1242, 1), (1275, 1), (1281, 2), (1341, 1), (1453, 1), (1518, 1), (1578, 1), (1604, 1), (1617, 1), (1633, 1), (1722, 1), (1731, 2), (1910, 1), (1920, 1), (1984, 1), (2013, 1), (2036, 1), (2077, 1), (2149, 1), (2211, 1), (2296, 1), (2318, 2), (2586, 1), (3029, 1), (3030, 1), (3200, 1), (3242, 1), (3290, 1), (3397, 1), (3839, 1), (3947, 1), (3984, 1), (4487, 1), (4974, 1), (5193, 1), (5225, 1), (5777, 1), (7175, 1), (7304, 1), (10177, 1), (14671, 1), (17834, 1), (23803, 1), (26091, 1), (29610, 2), (33976, 1), (39867, 1), (51851, 1), (51852, 1)]\n"
     ]
    }
   ],
   "source": [
    "print('Review (after pre-processing): ', text, '\\n')\n",
    "print('Review (after document tokenization, removing stopwords, lemmatization): ', text5, '\\n')\n",
    "print('Review (after gensim corpus): ', corpus[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframe of corpus, which tracks the restaurant that the review belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>restaurant_idx</th>\n",
       "      <th>yelp_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(17, 1), (26, 1), (34, 1), (42, 1), (72, 1), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(14, 1), (29, 1), (30, 1), (42, 1), (44, 1), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(23, 1), (26, 1), (42, 1), (60, 2), (61, 1), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(4, 1), (13, 1), (29, 1), (42, 2), (50, 2), (...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              corpus  restaurant_idx  \\\n",
       "0  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...               0   \n",
       "1  [(17, 1), (26, 1), (34, 1), (42, 1), (72, 1), ...               0   \n",
       "2  [(14, 1), (29, 1), (30, 1), (42, 1), (44, 1), ...               0   \n",
       "3  [(23, 1), (26, 1), (42, 1), (60, 2), (61, 1), ...               0   \n",
       "4  [(4, 1), (13, 1), (29, 1), (42, 2), (50, 2), (...               0   \n",
       "\n",
       "   yelp_rating  \n",
       "0            5  \n",
       "1            4  \n",
       "2            3  \n",
       "3            5  \n",
       "4            4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus = pd.DataFrame({'restaurant_idx':df_reviews['review_idx'], \n",
    "                          'corpus':corpus, \n",
    "                          'yelp_rating':df_reviews['rating']})\n",
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Basic word count and bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find most frequent words in best-rated and worst-rated Yelp restaurants\n",
    "\n",
    "- \"Good\" Yelp reviews have ratings = 5 \n",
    "- \"Bad\" Yelp reviews have ratings <= 3\n",
    "\n",
    "Note that individual reviews can only be an integer from 1 to 5. Overall average Yelp rating for a restaurant, however, is capable of increments of 0.5 (such as 4.5/5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best-rating reviews\n",
    "\n",
    "idx_good = df_reviews[df_reviews['rating'] == 5].index\n",
    "idx_good_doc = [t for t,j in df_corpus['restaurant_idx'].iteritems() if j in idx_good] # index of docs belonging to those restaurants\n",
    "\n",
    "subcorpus_good = []\n",
    "subcorpus_good = [(subcorpus_good + doc) for idx, doc in df_corpus.loc[idx_good_doc]['corpus'].iteritems()]\n",
    "\n",
    "# Worst-rating reviews\n",
    "\n",
    "idx_bad = df_reviews[df_reviews['rating'] <= 3 ].index\n",
    "idx_bad_doc = [t for t,j in df_corpus['restaurant_idx'].iteritems() if j in idx_bad] # index of docs belonging to those restaurants\n",
    "\n",
    "subcorpus_bad = []\n",
    "subcorpus_bad = [(subcorpus_bad + doc) for idx, doc in df_corpus.loc[idx_bad_doc]['corpus'].iteritems()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print top 10 words for \"good\" and \"bad\" Yelp reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for GOOD-rating Yelp reviews: \n",
      "\n",
      "food 40975\n",
      "good 32307\n",
      "place 27953\n",
      "dish 23071\n",
      "great 22255\n",
      "like 21997\n",
      "restaurant 21552\n",
      "one 20944\n",
      "service 19966\n",
      "would 19184\n",
      "\n",
      "\n",
      "Top 10 words for BAD-rating Yelp reviews: \n",
      "\n",
      "food 18129\n",
      "good 13151\n",
      "place 11688\n",
      "dish 10261\n",
      "restaurant 9890\n",
      "great 9536\n",
      "like 9255\n",
      "service 9052\n",
      "one 8562\n",
      "would 7879\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import itertools\n",
    "\n",
    "# Good-rating reviews\n",
    "\n",
    "total_word_count_good = collections.defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(subcorpus_good):\n",
    "    total_word_count_good[word_id] += word_count\n",
    "\n",
    "sorted_word_count_good = sorted(total_word_count_good.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "print('Top 10 words for GOOD-rating Yelp reviews:','\\n')\n",
    "for word_id, word_count in sorted_word_count_good[:10]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "print('\\n')\n",
    "\n",
    "# Bad-rating reviews\n",
    "\n",
    "total_word_count_bad = collections.defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(subcorpus_bad):\n",
    "    total_word_count_bad[word_id] += word_count\n",
    "\n",
    "sorted_word_count_bad = sorted(total_word_count_bad.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "print('Top 10 words for BAD-rating Yelp reviews:','\\n')\n",
    "for word_id, word_count in sorted_word_count_bad[:10]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>: As seen from the overlap between top-10 \"good\"/\"bad\" words from a simple bag of words count, we will need more sophisticated tools to parse keywords associated with \"good\" or \"bad\" ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. tf-idf EDA\n",
    "\n",
    "In the previous section, we did simple pre-processing and simply took token frequency. Here, we experiment with using gensim's [tf-idf](https://radimrehurek.com/gensim/models/tfidfmodel.html) to identify most important words in each document. This is accomplished with their algorithm by down-weighting shared words (between documents) beyond simply stopwords, ensuring that common words don't show up as key words. Conversely, document-specific words are weighted highly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Experimenting with tf-idf on a Yelp review\n",
    "\n",
    "We generate tfidf weights for a single document (Yelp review) to see how tf-idf performs. The tf-idf model is generated on the entire corpus of documents (i.e. reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf weights:  \n",
      " [(0, 0.11605235285752101), (1, 0.055517904959346144), (2, 0.058550703583362056), (3, 0.01923654735257033), (4, 0.027415914582950895)] \n",
      "\n",
      "Top 5 weighted words:\n",
      "oden 0.43697383680554125\n",
      "dashi 0.2652405475558776\n",
      "uh 0.2273065414231621\n",
      "mochi 0.19016016546668416\n",
      "spaghetti 0.1868451957856557\n",
      "\n",
      "\n",
      "Text:  \n",
      " davelle uh oden uh foodie trippin get order right uh shawty look good eatin oden oden dish drink dashi davelle oden moonlight xxxtentacion rip everything amazing u dining tiny cozy cramped beautiful little spot got oden set karaage cod spaghetti hokkaido spaghetti uni tomato cold dish topped kinda optional light cheese drink dashi aaaalllll dish good soft blanched skinless savory daikon served spicy yuzu paste use sparingly pretty big kick red miso paste soft mushy perfectly cooked heart shaped daikon mochi lightly fried bag soft gooey delicious mochi def drink dashi scallion enoki mushroom ginger hanpen white fish cake soft texture airy typical fish cake denseness fishcake delicious served spicy yuzu paste sausage served japanese mustard yummy bamboo shoot cooked enough left slight hate mushy bamboo shoot dashi similar taste mochi karaage soft juicy chicken lightly battered medium crisp cod spaghetti aka mentaiko pasta light cod roe taste fishy delicious hokkaido style spaghetti uni delicious much uni guess maybe another variation mentaiko liquor license sake soju cocktail beer wine went tonight quiet small space sure peak time usually wait\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "doc = corpus[0]\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print('tfidf weights: ', '\\n', tfidf_weights[:5], '\\n')\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "print('Top 5 weighted words:')\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary[term_id], weight)\n",
    "\n",
    "print('\\n')\n",
    "print('Text: ','\\n', ' '.join(corpus_tokenized[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>: It appears that if we take a document to be a single review, tf-idf may pick keywords that are specific to the reviewed restaurant's cuisine. Although this may be useful for identifying what food the restaurant serves, we are more interested in what the reviewer thought of the food, service, etc. \n",
    "\n",
    "Next, we try taking a document to be the the concatenation of all reviews belonging to a single restaurant to see if we get more relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Experimenting with tf-idf on all reviews belonging to a single restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf weights:  \n",
      " [(1, 0.018414406240671262), (2, 0.02010625861811797), (3, 0.006412573980437452), (4, 0.009325628439989457), (5, 0.017076277359615897)] \n",
      "\n",
      "Top 5 weighted words:\n",
      "oden 0.22861617999158015\n",
      "oden 0.1905134833263168\n",
      "gobo 0.1254834218545227\n",
      "oden 0.11430808999579008\n",
      "ada 0.10993338616073227\n"
     ]
    }
   ],
   "source": [
    "# Combine a single restaurant's reviews into one document (Davelle, first entry in df_nyt_yelp restaurant database)\n",
    "\n",
    "restaurant_idx = 0\n",
    "df = df_corpus[df_corpus['restaurant_idx']==0]\n",
    "doc = df['corpus'].tolist()\n",
    "doc = list(itertools.chain(*doc))\n",
    "\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print('tfidf weights: ', '\\n', tfidf_weights[:5], '\\n')\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "print('Top 5 weighted words:')\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary[term_id], weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion:</b> Top tf-idf keywords still seem to refer to the restaurant's type of cuisine more so than taste/food. It's likely that tf-idf, because it is designed to down-weight common words between documents, will actually leave out the phrases we want regarding food, service, and quality, since these are likely to appear across all documents (i.e. reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Summary\n",
    "\n",
    "We conclude this section with a pipeline setting up for converting documents (Yelp reviews) into token-wordcount mappings. As seen from the top wordcounts of \"best-rating\" and \"worst_rating\" Yelp reviews, there are many confounding terms that probably won't serve as good predictors for \"good\" or \"bad\" restaurants. \n",
    "\n",
    "Next, we'll experiment with word embeddings, sentiment analysis, CountVectorizer train-test-split on \"good\"/\"bad\" restaurants, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification: \"good\"/\"bad\" reviews\n",
    "\n",
    "Here, we use [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to train/predict review labels: \"good\" or \"bad\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Generating \"good\" & \"bad\" review labels\n",
    "\n",
    "Like the previous section, we'll take \"good\" ratings = 5 and \"bad\" ratings <= 3. As a result, we ignore reviews with a \"4\"-rating for now. \n",
    "\n",
    "The rationale is that these can contain a mix of positive/negative comments - negative comments explaining why the restaurant is not a 5, but also positive comments explaining why the restaurant would be > 3. Such a mix may confound our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate dataframe for classification train/test\n",
    "\n",
    "Generate dataframe we will be working with, which contains only reviews of <=3 & = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cool_count</th>\n",
       "      <th>elite_count</th>\n",
       "      <th>friend_count</th>\n",
       "      <th>funny_count</th>\n",
       "      <th>length_count</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_count</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_text</th>\n",
       "      <th>useful_count</th>\n",
       "      <th>user_count</th>\n",
       "      <th>review_idx</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>1771</td>\n",
       "      <td>5</td>\n",
       "      <td>345</td>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>Davelle, uh, oden, uh. Foodie, why you trippin...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jennie C.</td>\n",
       "      <td>0</td>\n",
       "      <td>davelle uh oden uh foodie trippin get order ri...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>486</td>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>2018-06-27</td>\n",
       "      <td>Lovely little 16 seater at the south end of th...</td>\n",
       "      <td>0</td>\n",
       "      <td>Adam W.</td>\n",
       "      <td>0</td>\n",
       "      <td>lovely little seater south end le went late lu...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>277</td>\n",
       "      <td>2</td>\n",
       "      <td>1583</td>\n",
       "      <td>5</td>\n",
       "      <td>96</td>\n",
       "      <td>2018-03-20</td>\n",
       "      <td>If you enjoy an small intimate cafe with diffe...</td>\n",
       "      <td>8</td>\n",
       "      <td>Maria S.</td>\n",
       "      <td>0</td>\n",
       "      <td>enjoy small intimate cafe different type japan...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cool_count  elite_count friend_count funny_count  length_count  rating  \\\n",
       "0          4            1           45           2          1771       5   \n",
       "3          1            1           55           0           486       5   \n",
       "6          2            1          277           2          1583       5   \n",
       "\n",
       "  review_count review_date                                        review_text  \\\n",
       "0          345  2018-06-18  Davelle, uh, oden, uh. Foodie, why you trippin...   \n",
       "3           58  2018-06-27  Lovely little 16 seater at the south end of th...   \n",
       "6           96  2018-03-20  If you enjoy an small intimate cafe with diffe...   \n",
       "\n",
       "  useful_count user_count  review_idx  \\\n",
       "0            4  Jennie C.           0   \n",
       "3            0    Adam W.           0   \n",
       "6            8   Maria S.           0   \n",
       "\n",
       "                                   text_preprocessed label  \n",
       "0  davelle uh oden uh foodie trippin get order ri...  good  \n",
       "3  lovely little seater south end le went late lu...  good  \n",
       "6  enjoy small intimate cafe different type japan...  good  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab \"good\" & \"bad\" review indices\n",
    "idx_class = idx_good.append(idx_bad)\n",
    "\n",
    "# Filter dataframe for only \"good\" and \"bad\" reviews. Save to 'df_class'\n",
    "df_class = df_reviews.loc[idx_class]\n",
    "\n",
    "# Include pre-processed text in new column: 'text_preprocessed'\n",
    "#text_preprocessed = [' '.join(doc) for doc in text_tokenized]\n",
    "#df = pd.DataFrame({'text':text_preprocessed})\n",
    "#df_class['text_preprocessed'] = text_preprocessed\n",
    "\n",
    "# Assign \"good\" or \"bad\" label\n",
    "df_class.loc[idx_good, 'label'] = 'good'\n",
    "df_class.loc[idx_bad, 'label'] = 'bad'\n",
    "\n",
    "# Remove reviews that are empty\n",
    "df_class[df_class['text_preprocessed'].isnull()]\n",
    "\n",
    "df_class.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 CountVectorizer for train/test split\n",
    "\n",
    "Use CountVectorizer to convert text to a sparse <b>document-term matrix (DTM)</b> of token counts, where each column is a <b>token</b> from the corpus vocabulary (generated from training set), each row is a <b>document</b> (a Yelp review), and the values are token frequency. Train & fit to set up for next section of predicting \"good\"/\"bad\" reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aaa', 'aaaaaaaaaand', 'aaaaaaaand', 'aaaaaamazing', 'aaaaahhhhhh', 'aaaaall', 'aaaaand', 'aaaalllll', 'aaaamazing', 'aaaand', 'aaamazing', 'aaanndd', 'aaawwweeesome', 'aahhhhh', 'aahhs', 'aahing', 'aahs', 'aaron', 'aarp']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df_class.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_class['text_preprocessed'], y, test_size=.33, random_state = 53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Learn the \"vocabulary\" of the training set & transform into 'document-term' matrix\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "#  Use the fitted vocabulary to build a DTM from the testing data (IGNORES tokens it hasn't seen before)\n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiny sample of resulting sparse DTM, where rows = Yelp reviews, columns = vocabulary generated from training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DTM:  (43897, 41596)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aloofness</th>\n",
       "      <th>alooot</th>\n",
       "      <th>alot</th>\n",
       "      <th>aloud</th>\n",
       "      <th>alp</th>\n",
       "      <th>alpha</th>\n",
       "      <th>alphabet</th>\n",
       "      <th>alphabetical</th>\n",
       "      <th>alphbet</th>\n",
       "      <th>alphonso</th>\n",
       "      <th>alpine</th>\n",
       "      <th>alpukat</th>\n",
       "      <th>alright</th>\n",
       "      <th>alrighty</th>\n",
       "      <th>alsace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aloofness  alooot  alot  aloud  alp  alpha  alphabet  alphabetical  \\\n",
       "0          0       0     0      0    0      0         0             0   \n",
       "\n",
       "   alphbet  alphonso  alpine  alpukat  alright  alrighty  alsace  \n",
       "0        0         0       0        0        0         0       0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Shape of DTM: ', count_train.shape)\n",
    "pd.DataFrame(count_train[0,1000:1015].toarray(), columns=count_vectorizer.get_feature_names()[1000:1015])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's apparent that there will be many duplicate versions/misspellings of a word, which might confound results and cause some terms to be downweighted in importance. Not clear how to correct for these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Naive Bayes classifier\n",
    "\n",
    "The Naive Bayes model is commonly used for testing NLP classification problems. It is rooted in probability ([Bayes theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem)) and generates predictions based on past data - given prior training data with features and labelled outcomes, what can we predict with our set of test observations and their features? The label it predicts for each observation is based on its calculation of the likeliest out of the possible labels. See [here](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/) for a good explanation.\n",
    "\n",
    "Here, each word from `CountVectorizer` acts as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Generate predicted \"good\"/\"bad\" reviews\n",
    "\n",
    "Use sklearn's [naive_bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) module to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8920956477498728 \n",
      "\n",
      "Confusion matrix: \n",
      " [[12397  1116]\n",
      " [ 1217  6891]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print('Accuracy score:', score, '\\n')\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['good', 'bad'])\n",
    "print('Confusion matrix:','\\n', cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier performed fairly well. Let's inspect the model to actually see what it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad [(-14.18594865371721, 'aaaaahhhhhh'), (-14.18594865371721, 'aarp'), (-14.18594865371721, 'aaverage'), (-14.18594865371721, 'abaiyin'), (-14.18594865371721, 'abandoning'), (-14.18594865371721, 'abate'), (-14.18594865371721, 'abbreviation'), (-14.18594865371721, 'abdominal'), (-14.18594865371721, 'abeille'), (-14.18594865371721, 'aberation'), (-14.18594865371721, 'abhorrently'), (-14.18594865371721, 'abiding'), (-14.18594865371721, 'abit'), (-14.18594865371721, 'abject'), (-14.18594865371721, 'abjectly'), (-14.18594865371721, 'ablaze'), (-14.18594865371721, 'abnormal'), (-14.18594865371721, 'abnoxious'), (-14.18594865371721, 'aboard'), (-14.18594865371721, 'abodanza')] \n",
      "\n",
      "good [(-5.553999225002784, 'love'), (-5.546537829576723, 'come'), (-5.510385126329531, 'try'), (-5.490441926904557, 'experience'), (-5.416130781664593, 'definitely'), (-5.4130285610907425, 'meal'), (-5.171015078083638, 'really'), (-5.159651319433323, 'amazing'), (-5.157489935306597, 'best'), (-5.121790791919109, 'menu'), (-5.066956321200333, 'time'), (-5.031226705504039, 'like'), (-5.003802016111809, 'delicious'), (-4.95786660026487, 'service'), (-4.923870086336613, 'dish'), (-4.904683943404329, 'restaurant'), (-4.765347356257831, 'good'), (-4.661746897505944, 'great'), (-4.577705025840153, 'place'), (-4.237678958409189, 'food')]\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20], '\\n')\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results make sense. Negative words such as \"abberation\" and \"abominable\" feature prominently in \"bad\" reviews, while \"good\" reviews have positive descriptors such as \"delicious\" and \"amazing\". \n",
    "\n",
    "It is interesting to note features that by themselves are neutral, such as \"service\". Since they are included in \"good\" reviews, it seems service is an important factor and is conducted well in \"good\" reviews.\n",
    "\n",
    "However, it seems like reviewer misspellings could downweight the term's importance (ex. \"abnoxious\" vs \"obnoxious\"). There are a few duplicate words (ex. \"abberation\", \"aberation\") to the same effect as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Examine misclassified restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check examples of false positives\n",
    "\n",
    "Where \"bad\" reviews were incorrectly classified as \"good\" reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 example:  \n",
      "\n",
      "Pre-processed:  great bistro ambiance bistro actual building gorgeous feel much like sitting nice bistro lyon nyc high end price view street delivery truck service fantastic gourgeres bar nice touch food good priced great ambiance crowded \n",
      "\n",
      "Actual review:  Great Bistro ambiance, at not so bistro prices.. . The actual building is gorgeous and feels very much like you are sitting in a nice bistro in Lyon, but with NYC high end prices, and views of 55th street delivery trucks and trashbags.. . The service is fantastic, and the gourgeres at the bar are a nice touch. The food is very good but over priced for what you get.. . Great ambiance when not crowded. \n",
      "\n",
      "First 10 examples: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78019    loved room atmosphere waitress world class bil...\n",
       "63215    good need go back love marcus last time nyc sa...\n",
       "89555    done like old tavern miss dearly think formula...\n",
       "31135    stopped quick lunch special fired tofu pepper ...\n",
       "67796    wife showed sunday night dinner seated pretty ...\n",
       "17655    perhaps ordered wrong thing indian spiced vege...\n",
       "52479    food ordinary idea owner book decoration brigh...\n",
       "9570     food pretty good got burrata best ever still g...\n",
       "9888     place conveniently located near grove street p...\n",
       "28132    superb french food simply unique good service ...\n",
       "Name: text_preprocessed, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives = X_test[y_test < pred]\n",
    "print('1 example: ','\\n')\n",
    "print('Pre-processed: ', false_positives[47597], '\\n')\n",
    "print('Actual review: ', df_reviews.loc[47597, 'review_text'], '\\n')\n",
    "print('First 10 examples: ')\n",
    "false_positives.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, a likely reason some reviews were false positives was due to the prevalence of positive words (ex. \"good\", \"gorgeous\") in the midst of a negative evaluation with a few turns of phrase (ex. \"...but over priced for what you get\"). \n",
    "\n",
    "In fact, in our example, pre-processing and removing stopwords may have removed tokens that would have caused the review to be correctly labelled as \"bad\", since they were critical parts of negative turns of phrase. Phrases/words such as \"not so\", \"trashbags\", \"but over priced for what you get\" are lost as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check examples of false negatives\n",
    "\n",
    "Where \"good\" reviews were incorrectly classified as \"bad\" reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 example:  \n",
      "\n",
      "Pre-processed:  love place went first time ordered salt pepper chicken rice tofu vegetable rice beef tendon dish rice stayed meal complimentary pork soup tea nice soup tea like place mama lee made soup tea heart kindness know restarurant complimentary thing taste kinda bland shitty sometimes felt like home eating meal rice came separately another bowl even tho ordered rice worth \n",
      "\n",
      "Actual review:  Love this place. I went there for the first time and ordered salt and pepper chicken over rice, tofu with vegetable over rice,  and beef tendon dish over rice. We stayed for the meal and they have complimentary pork soup and tea. . . It's a very nice soup and tea, this is not like other places, Mama Lee made these soup and tea with her heart (kindness). You know how other restarurant complimentary things taste kinda just bland and shitty sometimes, this is not at all!   I felt like home eating all the meals. . . The rice came separately with another bowl even tho you ordered \"______over rice\"  . . It is worth it. \n",
      "\n",
      "First 10 examples: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4870     grey haired waiter super cute service good cas...\n",
       "798      got excited heard place specialized taiwanese ...\n",
       "32283    came dine tavern bar could specifically famous...\n",
       "31240    new favorite chinese restaurant town term comi...\n",
       "57858    vegan really appreciate burger include thick c...\n",
       "39689    ordered seafood tofu casserole mushroom noodle...\n",
       "71694    recent rediscovered shuko scallop good first t...\n",
       "7990     cold brew great decently priced even give nyu ...\n",
       "23423    inside chelsea market known beef noodle soup n...\n",
       "4163     waiting review cote week different time recent...\n",
       "Name: text_preprocessed, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives = X_test[y_test > pred]\n",
    "\n",
    "print('1 example: ','\\n')\n",
    "print('Pre-processed: ', false_negatives[18056],'\\n')\n",
    "print('Actual review: ', df_reviews.loc[18056, 'review_text'], '\\n')\n",
    "print('First 10 examples: ')\n",
    "false_negatives.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the false positives, these reviews tend to have a mixed bag of vocabulary associated with both \"good\" & \"bad\" reviews. In the above example for instance, there are many positive words (ex. \"love\", \"nice\") but also negative words that would probably trigger a \"bad\" review (ex. \"shitty\", \"bland\") even though the reviewer used these terms to describe other competing restaurants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Generate list of misclassified restaurants\n",
    "\n",
    "Set up for next section - we're interested in whether we can actually \"predict\" misclassified restaurants, using NYT data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices for misclassified reviews, false positives (fp) & false negatives (fn)\n",
    "fp_idx = false_positives.index.values.tolist()\n",
    "fn_idx = false_negatives.index.values.tolist()\n",
    "\n",
    "# get fp/fn restaurants\n",
    "fp_rest = [df_reviews.loc[idx]['review_idx'] for idx in fp_idx]\n",
    "fn_rest = [df_reviews.loc[idx]['review_idx'] for idx in fn_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Influence of NYT on NLP results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous notebook (`df_NYT_influence`), we integrated NYT and Yelp information to determine whether NYT dining reviews had an influence on Yelp ratings. We found some possible effects, but we'd like to refine our investigation further by leveraging the NLP techniques in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Revisting tf-idf\n",
    "\n",
    "Previously, we found that tf-idf tended to highlight cuisine-specific words in Yelp reviews. Although this may not have been helpful for classifying \"good\"/\"bad\" reviews, it may allow us to determine whether mentions of specific dishes in NYT reviews influence their frequency in Yelp reviews (i.e. people ordered it more after reading about it in NYT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating pre- & post-NYT Yelp reviews\n",
    "\n",
    "As we did in the `df_NYT_influence` notebook, we use a 1-year window pre- and post-NYT to generate two datasets - control group w/o possible NYT review effects and test group in which NYT effects might be observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "# import our results from \"df_NYT_influence\" notebook\n",
    "\n",
    "df_influence = pd.read_pickle('import/df_influence.pkl')\n",
    "\n",
    "# generate documents: a pre-NYT and post-NYT\n",
    "\n",
    "corpus_before_total = []\n",
    "corpus_after_total = []\n",
    "\n",
    "for idx, row in df_influence.iterrows():\n",
    "    \n",
    "    # extract all reviews for restaurant\n",
    "    nyt_review_time = row['nyt_review_time']\n",
    "    df_reviews[df_reviews['review_idx']==idx]\n",
    "    df = df_reviews[df_reviews['review_idx']==idx]\n",
    "    \n",
    "    # extract pre- & post-NYT reviews\n",
    "    df_before_idx = df[df['review_date'] <= nyt_review_time].index.values.tolist()\n",
    "    df_after_idx = df[df['review_date'] > nyt_review_time].index.values.tolist()\n",
    "    \n",
    "    # extract tokenized version\n",
    "    \n",
    "    corpus_before = df_corpus.loc[df_before_idx]['corpus']\n",
    "    corpus_before = list(itertools.chain(*corpus_before))\n",
    "    if not corpus_before:\n",
    "        corpus_before = np.nan\n",
    "    corpus_before_total.append(corpus_before)\n",
    "    \n",
    "    corpus_after = df_corpus.loc[df_after_idx]['corpus']\n",
    "    corpus_after = list(itertools.chain(*corpus_after))\n",
    "    if not corpus_after:\n",
    "        corpus_after = np.nan\n",
    "    corpus_after_total.append(corpus_after)\n",
    "    \n",
    "# append results onto tfidf dataframe\n",
    "\n",
    "df_influence['before_text'] = corpus_before_total\n",
    "df_influence['after_text']  = corpus_after_total\n",
    "\n",
    "# drop restaurants w/o \"before\" & \"after\" data\n",
    "df_influence2 = df_influence.drop(df_influence[df_influence['before_text'].isnull()].index.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform tfidf on pre- & post-NYT Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critics_pick</th>\n",
       "      <th>mean_after</th>\n",
       "      <th>mean_before</th>\n",
       "      <th>mean_diff</th>\n",
       "      <th>nyt_name</th>\n",
       "      <th>nyt_review_time</th>\n",
       "      <th>nyt_stars</th>\n",
       "      <th>pval</th>\n",
       "      <th>reviewcount</th>\n",
       "      <th>yelp_rating</th>\n",
       "      <th>noboot_after</th>\n",
       "      <th>noboot_before</th>\n",
       "      <th>noboot_diff</th>\n",
       "      <th>count_before</th>\n",
       "      <th>count_after</th>\n",
       "      <th>count_difference</th>\n",
       "      <th>before_text</th>\n",
       "      <th>after_text</th>\n",
       "      <th>before_tfidf</th>\n",
       "      <th>after_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>-0.833333</td>\n",
       "      <td>Davelle</td>\n",
       "      <td>2018-06-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.544</td>\n",
       "      <td>47.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>-0.833333</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>-5.166667</td>\n",
       "      <td>[(4, 1), (13, 1), (29, 1), (42, 2), (50, 2), (...</td>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n",
       "      <td>[(757, 0.10386762434632926), (72, 0.0964596267...</td>\n",
       "      <td>[(23, 0.19573994069456446), (117, 0.1810946375...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>Lahi</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.527</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>[(14, 1), (19, 1), (26, 1), (29, 3), (35, 1), ...</td>\n",
       "      <td>[(42, 1), (129, 1), (164, 1), (176, 1), (188, ...</td>\n",
       "      <td>[(1231, 0.17693062409696278), (849, 0.14468225...</td>\n",
       "      <td>[(1374, 0.3021073373234878), (1147, 0.24844764...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  critics_pick  mean_after  mean_before  mean_diff nyt_name nyt_review_time  \\\n",
       "0            y        3.50     4.333333  -0.833333  Davelle      2018-06-07   \n",
       "1            n        4.25     4.125000   0.125000     Lahi      2018-05-31   \n",
       "\n",
       "   nyt_stars   pval  reviewcount  yelp_rating  noboot_after  noboot_before  \\\n",
       "0        NaN  0.544         47.0          4.0          3.50       4.333333   \n",
       "1        NaN  0.527         34.0          4.0          4.25       4.125000   \n",
       "\n",
       "   noboot_diff  count_before  count_after  count_difference  \\\n",
       "0    -0.833333      6.500000     1.333333         -5.166667   \n",
       "1     0.125000      2.666667     0.666667         -2.000000   \n",
       "\n",
       "                                         before_text  \\\n",
       "0  [(4, 1), (13, 1), (29, 1), (42, 2), (50, 2), (...   \n",
       "1  [(14, 1), (19, 1), (26, 1), (29, 3), (35, 1), ...   \n",
       "\n",
       "                                          after_text  \\\n",
       "0  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n",
       "1  [(42, 1), (129, 1), (164, 1), (176, 1), (188, ...   \n",
       "\n",
       "                                        before_tfidf  \\\n",
       "0  [(757, 0.10386762434632926), (72, 0.0964596267...   \n",
       "1  [(1231, 0.17693062409696278), (849, 0.14468225...   \n",
       "\n",
       "                                         after_tfidf  \n",
       "0  [(23, 0.19573994069456446), (117, 0.1810946375...  \n",
       "1  [(1374, 0.3021073373234878), (1147, 0.24844764...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform tfidf to get weights for pre- & post-NYT reviews\n",
    "\n",
    "corpus = df_corpus.loc[df_influence.index.values.tolist()]['corpus'].tolist() # grabbing corpus info for these restaurants\n",
    "tfidf = TfidfModel(corpus) # generating tfidf object\n",
    "\n",
    "tfidf_weights_before = []\n",
    "for doc in df_influence2['before_text'].tolist():\n",
    "    tfidf_weights = tfidf[doc]\n",
    "    tfidf_weights_before.append(tfidf_weights)\n",
    "tfidf_weights_before = [sorted(doc_weights, key=lambda w: w[1], reverse=True) for doc_weights in tfidf_weights_before] # sort\n",
    "df_influence2['before_tfidf'] = tfidf_weights_before\n",
    "    \n",
    "tfidf_weights_after = []\n",
    "for doc in df_influence2['after_text'].tolist():\n",
    "    tfidf_weights = tfidf[doc]\n",
    "    tfidf_weights_after.append(tfidf_weights)\n",
    "tfidf_weights_after = [sorted(doc_weights, key=lambda w: w[1], reverse=True) for doc_weights in tfidf_weights_after] # sort\n",
    "df_influence2['after_tfidf'] = tfidf_weights_after\n",
    "\n",
    "df_influence2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Make a NYT review text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenized_NYT = []\n",
    "text_preprocessed_NYT = []\n",
    "\n",
    "for idx, review in df_nyt_yelp.iterrows():\n",
    "    \n",
    "    # basic pre-processing\n",
    "    text = review['nyt_text'].lower() # lowercase doc\n",
    "    #text = str(TextBlob(text).correct())\n",
    "    \n",
    "    text2 = word_tokenize(text) # tokenize doc\n",
    "    text3 = [tok for tok in text2 if tok.isalpha()] # retain only alphabetic words\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english')) # generate stopwords from English dictionary\n",
    "    text4 = [tok for tok in text3 if tok not in stop_words]\n",
    "    \n",
    "    # lemmatize tokens\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text5 = [wordnet_lemmatizer.lemmatize(tok) for tok in text4]\n",
    "    \n",
    "    text6 = ' '.join(text5)\n",
    "    \n",
    "    text_tokenized_NYT.append(text5)\n",
    "    text_preprocessed_NYT.append(text6)\n",
    "\n",
    "df_nyt_yelp['nyt_text_preprocessed'] = text_preprocessed_NYT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dictionary and bag of words for NYT reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_NYT = Dictionary(text_tokenized_NYT)\n",
    "\n",
    "# generate corpus\n",
    "corpus_NYT = [dictionary.doc2bow(doc) for doc in text_tokenized_NYT] # .doc2bow method converts documents into BoW format\n",
    "corpus_NYT = [sorted(doc, key=lambda w: w[1], reverse=True) for doc in corpus_NYT]\n",
    "df_nyt_yelp['nyt_text_tokenized'] = corpus_NYT\n",
    "# generate BoW\n",
    "\n",
    "nyt_bow_total = []\n",
    "for idx, restaurant in df_nyt_yelp.iterrows(): \n",
    "    \n",
    "    doc = restaurant['nyt_text_tokenized'] # retrieve tokenized review\n",
    "    word_id_total = []\n",
    "    word_count_total = []\n",
    "    for word_id, word_count in doc:\n",
    "        word_id_total.append(dictionary_NYT.get(word_id))\n",
    "        word_count_total.append(word_count)\n",
    "    nyt_bow = {'word':word_id_total, 'count':word_count_total}\n",
    "    nyt_bow_total.append(nyt_bow)\n",
    "\n",
    "df_nyt_yelp['nyt_bow'] = nyt_bow_total\n",
    "del nyt_bow_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform tfidf to get weights for NYT reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfModel(corpus) # generating tfidf object\n",
    "\n",
    "tfidf_weights_total = []\n",
    "for idx, restaurant in df_nyt_yelp.iterrows():\n",
    "    doc = restaurant['nyt_text_tokenized']\n",
    "    tfidf_weights = tfidf[doc]\n",
    "    tfidf_weights_total.append(tfidf_weights)\n",
    "tfidf_weights_total= [sorted(doc_weights, key=lambda w: w[1], reverse=True) for doc_weights in tfidf_weights_total] # sort\n",
    "\n",
    "# find word-match for tokens\n",
    "\n",
    "nyt_tfidf_total = []\n",
    "for doc in tfidf_weights_total:\n",
    "    tokens = [word_id for word_id, word_count in doc]\n",
    "    words = [dictionary.get(t) for t in tokens]\n",
    "    weights = [word_count for word_id, word_count in doc]\n",
    "    nyt_tfidf = {\"words\":words, \"weights\":weights}  \n",
    "    nyt_tfidf_total.append(nyt_tfidf)\n",
    "    \n",
    "df_nyt_yelp['nyt_tfidf'] = nyt_tfidf_total\n",
    "\n",
    "del tfidf_weights; del tokens; del words; del weights\n",
    "\n",
    "df_nyt_yelp.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Determine NYT-Yelp word intersection (i.e. shared words)\n",
    "#### Determine intersection of NYT review words w/ Yelp reviews, pre- vs. post-NYT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_intersect_total = []\n",
    "after_intersect_total = []\n",
    "before_intersect_count_total = []\n",
    "after_intersect_count_total = []\n",
    "for idx, restaurant in df_influence2.iterrows():\n",
    "    \n",
    "    before_tfidf = restaurant['before_tfidf'] # tfidf weights for pre-NYT Yelp reviews\n",
    "    after_tfidf = restaurant['after_tfidf'] # tfidf weights for post-NYT Yelp reviews\n",
    "    #nyt_bow = df_nyt_yelp.loc[idx]['nyt_bow'] # retrieving NYT bag of words\n",
    "    nyt_words = df_nyt_yelp.loc[idx]['nyt_tfidf']['words'][:100]\n",
    "    \n",
    "    # retrieve Yelp review words (already sorted from most to least word count)\n",
    "    tokens_before = [word_id for word_id, word_count in before_tfidf] # before\n",
    "    words_before = [dictionary.get(t) for t in tokens_before]\n",
    "    tokens_after = [word_id for word_id, word_count in after_tfidf] # before\n",
    "    words_after = [dictionary.get(t) for t in tokens_after]   \n",
    "    \n",
    "    # take intersection of NYT words and top 100 words pre- & post-NYT\n",
    "    before = set(words_before[:20])\n",
    "    after = set(words_after[:20])\n",
    "    #nyt = set(nyt_bow['word'])\n",
    "    nyt = set(nyt_words)\n",
    "    before_intersect = set.intersection(before, nyt) # intersection of NYT words and Yelp review top-100 words\n",
    "    after_intersect = set.intersection(after, nyt)\n",
    "    \n",
    "    before_intersect_total.append(before_intersect)\n",
    "    after_intersect_total.append(after_intersect)\n",
    "    before_intersect_count_total.append(len(before_intersect)) # count of # of intersections\n",
    "    after_intersect_count_total.append(len(after_intersect))\n",
    "\n",
    "df_influence2['before_intersect'] = before_intersect_total\n",
    "df_influence2['after_intersect'] = after_intersect_total\n",
    "df_influence2['before_intersect_total'] = before_intersect_count_total\n",
    "df_influence2['after_intersect_total'] = after_intersect_count_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results: NYT & Yelp intersecting terms, pre- & post-NYT.\n",
    "\n",
    "Our calculation for post-NYT increase in intersecting terms is: (difference)/(# of pre-NYT intersecting terms), where difference = (# of post-NYT words) - (# of pre-NYT words). So a 1-fold increase = doubling of intersecting terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1373d730>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Index of restaurants whose intersecting terms w/ NYT review is greater post-NYT vs. pre-\n",
    "#idx_greaterafter = [idx for idx, row in df_influence2.iterrows() if row['after_intersect_total']>row['before_intersect_total']]\n",
    "# df = df_influence2.loc[idx_greaterafter]\n",
    "\n",
    "# Calculate difference, pre- vs. post-NYT\n",
    "difference = []\n",
    "for idx, row in df_influence2.iterrows():\n",
    "    if row['before_intersect_total']==0:\n",
    "        d = row['after_intersect_total']\n",
    "    else:\n",
    "        d = (row['after_intersect_total'] - row['before_intersect_total'])/row['before_intersect_total']\n",
    "    difference.append(d)\n",
    "df_influence2['difference_intersect'] = difference\n",
    "\n",
    "# Visualize distribution\n",
    "plt.hist(difference);\n",
    "plt.title('Intersecting NYT & Yelp terms: pre- vs. post-NYT');\n",
    "plt.xlabel('n-Fold times increase, post-NYT');\n",
    "plt.ylabel('# of restaurants');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The post-NYT increases in intersecting NYT & Yelp words varies. In the majority of cases, the n-fold increase is < 1. \n",
    "- It's noteworthy that for those restaurants in which a <i>decrease</i> in intersecting NYT-Yelp terms occurred, it's never greater than 1. This means NYT reviews did not cause any unusual n-fold decrease in intersecting terms - such an anomoly would  might undermine the validity of using NYT reviews to evaluate its role in publicizing specific dishes.\n",
    "\n",
    "Let's focus on those w/ >= 0.5-fold increases (an increase of intersecting terms of at least 1/2 as many as before, post-NYT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Good\" NYT ratings - influence on n-fold increases?\n",
    "\n",
    "Are \"good\" NYT ratings (Critic's Pick and/or NYT stars) more likely to experience >=0.5-fold increase in shared NYT & Yelp words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% restaurants w/ 'good' ratings (Critic's Pick and/or NYT stars)\n",
      ">=1-fold increase, post-NYT:  0.7681159420289855\n",
      "<1-fold increase, post-NYT:  0.6979591836734694 \n",
      "\n",
      "Number of restaurants w/ \"good\" ratings:  69\n"
     ]
    }
   ],
   "source": [
    "df1 = df_influence2[df_influence2['difference_intersect']>=.5]\n",
    "df2 = df_influence2[df_influence2['difference_intersect']<.5]\n",
    "\n",
    "print(\"% restaurants w/ 'good' ratings (Critic's Pick and/or NYT stars)\")\n",
    "print(\">=1-fold increase, post-NYT: \", len(df1[ (df1['nyt_stars']>3) | (df1['critics_pick']=='y')  ])/len(df1))\n",
    "print(\"<1-fold increase, post-NYT: \", len(df2[ (df2['nyt_stars']>3) | (df2['critics_pick']=='y')  ])/len(df2),'\\n')\n",
    "\n",
    "print('Number of restaurants w/ \"good\" ratings: ', len(df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few samples of such changes in intersecting terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davelle: \n",
      "\n",
      "pre-NYT shared terms:  {'oden'}\n",
      "post-NYT shared terms:  {'daikon', 'uni', 'spaghetti', 'urchin', 'dashi', 'hokkaido', 'mentaiko', 'oden'} \n",
      "\n",
      "Rangoon Spoon: \n",
      "\n",
      "pre-NYT shared terms:  {'noodle', 'burmese'}\n",
      "post-NYT shared terms:  {'spoon', 'tofu', 'rangoon', 'thoke', 'burmese'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Davelle: \\n')\n",
    "print('pre-NYT shared terms: ', df_influence2.loc[0]['before_intersect'])\n",
    "print('post-NYT shared terms: ', df_influence2.loc[0]['after_intersect'],'\\n')\n",
    "\n",
    "print('Rangoon Spoon: \\n')\n",
    "print('pre-NYT shared terms: ', df_influence2.loc[3]['before_intersect'])\n",
    "print('post-NYT shared terms: ', df_influence2.loc[3]['after_intersect'],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Conclusion\n",
    "Among the restaurants w/ at least a 0.5-fold, post-NYT increase in NYT-Yelp shared words, ~76% had a \"good\" NYT review. Conversely, among <0.5-fold increase restaurants (i.e. insignificant post-NYT change), ~70% had a \"good' NYT review.  \n",
    "\n",
    "The difference may seem significant, but we should keep in mind that the sample size per a group is very asymmetrical (numbers below). As a result, we perform a permutation test in the next section to attempt a test for significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"good\"-NYT reviewed restaurants:  224\n",
      "Number of \"bad\"-NYT reviewed restaurants:  60 \n",
      "\n",
      "Number of >=0.5-fold increased restaurants:  69\n",
      "Number of <0.5-fold increased restaurants:  245\n"
     ]
    }
   ],
   "source": [
    "df_good = df_influence2[(df_influence2['nyt_stars']>3) | (df_influence2['critics_pick']=='y')]\n",
    "df_bad = df_influence2[(df_influence2['critics_pick']=='n') & (df_influence2['nyt_stars'].isnull()==True)]\n",
    "\n",
    "print('Number of \"good\"-NYT reviewed restaurants: ', len(df_good))\n",
    "print('Number of \"bad\"-NYT reviewed restaurants: ', len(df_bad), '\\n')\n",
    "\n",
    "print('Number of >=0.5-fold increased restaurants: ', len(df1))\n",
    "print('Number of <0.5-fold increased restaurants: ', len(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Permutation test - NYT-Yelp shared words\n",
    "\n",
    "Since our distribution of NYT-Yelp word intersections is non-normal, we use a nonparametric test called the [permutation test](https://www2.stat.duke.edu/~ar182/rr/examples-gallery/PermutationTest.html), which is similiar to bootstrapping. \n",
    "- \"Control\" group - the bad/neutral NYT reviewed restaurants, in which no change in post-NYT NYT-Yelp shared words would be expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 \"Good\" NYT restaurants - stars and/or \"Critic's Pick\"\n",
    "\n",
    "We use the same classification for a \"good\" NYT review that we used in section 4: either receiving stars and/or the \"Critic's Pick\" label. Our two groups are: 1) \"good\"-NYT reviewed restaurants, 2) neutrally-reviewed restaurants in which neither a \"Critic's Pick\" nor NYT stars were awarded.\n",
    "\n",
    "#### Set up permutation test and function. \n",
    "\n",
    "Function for permutation test (adapted from [here](https://www2.stat.duke.edu/~ar182/rr/examples-gallery/PermutationTest.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation function\n",
    "def run_permutation_test(pooled,sizeZ,sizeY):\n",
    "    np.random.shuffle(pooled)\n",
    "    starZ = pooled[:sizeZ]\n",
    "    starY = pooled[-sizeY:]\n",
    "    return starZ.mean() - starY.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between control & expt group:  0.10468891723356008\n"
     ]
    }
   ],
   "source": [
    "# separate data into 2 groups\n",
    "z = df_good['difference_intersect'] # possibily \"significant\" pre- vs. post-NYT differences in # of shared words\n",
    "y = df_bad['difference_intersect'] # \"control\" group w/ no significant difference\n",
    "\n",
    "# calculate diff between the 2 groups\n",
    "delta = z.mean() - y.mean()\n",
    "print('Difference between control & expt group: ', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25370000000000004"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled = np.hstack([z,y])\n",
    "numSamples = 10000\n",
    "estimates = np.array(list(map(lambda x: run_permutation_test(pooled,z.size,y.size),range(numSamples))))\n",
    "diffCount = len(np.where(estimates >=delta)[0])\n",
    "hat_asl_perm = 1.0 - (float(diffCount)/float(numSamples))\n",
    "hat_asl_perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, since our p-value is > 0.05, we fail to reject the null hypothesis that there is no difference between the \"control\" (i.e. group in which there is no significant pre- vs. post-NYT change in # of word intersections) and \"experimental\" group (w/ >=0.5-fold post-NYT change)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous notebook (`data_NYT_influence`), we found that restaurants w/ positive NYT reviews were slightly more likely to experience in increase in average Yelp rating afterwards. Since the results were less than expected, we also explored impact on Yelp popularity and found a larger increase. \n",
    "\n",
    "There were undoubtly other features at play, and here we explore whether NYT influenced the language of Yelp reviews. More specifically, would mentions of certain dishes in a NYT review increase mentions in Yelp reviews afterwards (i.e. implying that more people ordered the dish)? We used <i>tfidf</i> to improve our hits of cuisine-specific terms and compared Yelp reviews, pre- & post-NYT review publication. \n",
    "\n",
    "<b>Results</b>: Setting a threshold of 0.5-fold for the post-NYT increase in shared NYT-Yelp words, we initially had promising results: among restaurants experiencing at least a 0.5-fold post NYT increase, 76% had a \"good\" NYT review (vs. 70% for <0.5-fold increase). This would seem to suggest that having a positive NYT review would increase the likelihood that Yelp reviews would experience a post-NYT boost in shared, mostly dish-specific terms.\n",
    "\n",
    "However, upon doing a significance test (permutation test), we could not find this difference to be significant. Thus, we cannot conclude that a positive NYT review will impact Yelp reviews, at least language-wise.\n",
    "\n",
    "In the process, we: \n",
    "- <b>Set up a pipeline for NLP</b> - including pre-processing, generating a corpus, bag of words, CountVectorizer sparse DTM, etc. \n",
    "- <b>Classified \"good\" & \"bad\" reviews</b> - Discovered words (and word frequencies) associated with \"good\" & \"bad\" reviews. Successfully explored classification for \"good\" & \"bad\" reviews. Our Naive Bayes classifier worked fairly well, with an accuracy score of ~90%. \n",
    "- <b>Explored the influence of NYT reviews on Yelp review language</b> - Set up a method for determining whether restaurants receiving positive NYT reviews would be more likely to experience a greater number of shared words between NYT and Yelp reviews, after the NYT reviews were published.\n",
    "- <b>NYT-Yelp shared words significance test</b> - Applied permutation test to determine whether the difference between pre- & post-NYT NYT-Yelp shared words was significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
