{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl \n",
    "%matplotlib inline\n",
    "mpl.rcParams['patch.force_edgecolor'] = True\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nyt_yelp = pd.read_pickle('df_nyt_yelp_corrected.pkl') # import nyt-yelp master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews:  72007\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cool_count</th>\n",
       "      <th>elite_count</th>\n",
       "      <th>friend_count</th>\n",
       "      <th>funny_count</th>\n",
       "      <th>length_count</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_count</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_text</th>\n",
       "      <th>useful_count</th>\n",
       "      <th>user_count</th>\n",
       "      <th>review_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>1771</td>\n",
       "      <td>5</td>\n",
       "      <td>345</td>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>Davelle, uh, oden, uh. Foodie, why you trippin...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jennie C.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>791</td>\n",
       "      <td>4</td>\n",
       "      <td>350</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>Keep It Simple Smart.  Cute all day cafe with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yvonne C.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cool_count  elite_count friend_count funny_count  length_count  rating  \\\n",
       "0          4            1           45           2          1771       5   \n",
       "1          1            1           68           0           791       4   \n",
       "\n",
       "  review_count review_date                                        review_text  \\\n",
       "0          345  2018-06-18  Davelle, uh, oden, uh. Foodie, why you trippin...   \n",
       "1          350  2018-07-01  Keep It Simple Smart.  Cute all day cafe with ...   \n",
       "\n",
       "  useful_count user_count  review_idx  \n",
       "0            4  Jennie C.           0  \n",
       "1            1  Yvonne C.           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from dateutil import parser\n",
    "\n",
    "# make a new, second master dataframe that compiles all reviews scraped so far\n",
    "df_reviews = pd.DataFrame()\n",
    "rest_length = []\n",
    "for idx in range(0,280):\n",
    "    pkl_name = 'import/restaurants/' + str(idx) + '.pkl' # get Pickle file name of restaurant\n",
    "    df = pd.read_pickle(pkl_name) # import pickled dataframe\n",
    "    df['review_idx'] = idx # affix restaurant's index to dataframe\n",
    "    df_reviews = df_reviews.append(df) # append to master dataframe of reviews\n",
    "    rest_length.append(len(df))\n",
    "\n",
    "df_reviews = df_reviews.reset_index().drop('index', axis=1)\n",
    "\n",
    "# convert date to DateTime object\n",
    "df_reviews['review_date'] = [parser.parse(t) for t in df_reviews['review_date']]\n",
    "#df_reviews['review_date'] = [datetime.strptime(str(t).split()[0], '%Y-%m-%d') for t in df_reviews['review_date']]\n",
    "\n",
    "# convert rating from str to numeric\n",
    "df_reviews['rating'] = pd.to_numeric(df_reviews['rating'])\n",
    "\n",
    "print('Total number of reviews: ', len(df_reviews))\n",
    "df_reviews.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.DataFrame({'yelp_name':df_nyt_yelp['yelp_name'][:238], 'length':rest_length})\n",
    "#test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop invalid restaurants\n",
    "\n",
    "Drop some other restaurants that were discovered to be invalid (not from NYC, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_idx = [34,44,47,151,352,498]\n",
    "df_nyt_yelp = df_nyt_yelp.drop(delete_idx)\n",
    "\n",
    "delete_reviews = [idx for idx, row in df_reviews.iterrows() if row['review_idx'] in delete_idx]\n",
    "df_reviews = df_reviews.drop(delete_reviews).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill in master dataframe w/ scraped reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab restaurants that have complete Yelp scraped data now\n",
    "#df_master = df_nyt_yelp.iloc[:`90]\n",
    "\n",
    "# fill in Yelp scraped data\n",
    "#for idx in range(0,88):\n",
    "#    pkl_name = str(idx) + '.pkl'\n",
    "#    df = pd.read_pickle(pkl_name)\n",
    "#    df_master.loc[idx, 'yelp_reviews'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Yelp reviews - NLP corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 NLP pre-processing\n",
    "\n",
    "Generate a pre-processed, tokenized list of documents in preparation for using gensim to create a corpus. Here, a <u>document</u> = a review's text. Each document is converted to a list of pre-processed tokens (not unique - will list all instances of tokens).\n",
    "\n",
    "Pre-processing steps: \n",
    "- Lowercase\n",
    "- Remove non-alphabetic characters/punctuation\n",
    "- Remove stop words\n",
    "- Lemmatize\n",
    "- Correct (some) misspellings w/ [TextBlob](http://textblob.readthedocs.io/en/dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "text_tokenized = []\n",
    "text_preprocessed = []\n",
    "\n",
    "for idx, review in df_reviews.iterrows():\n",
    "    \n",
    "    # basic pre-processing\n",
    "    text = review['review_text'].lower() # lowercase doc\n",
    "    text = str(TextBlob(text).correct())\n",
    "    \n",
    "    text2 = word_tokenize(text) # tokenize doc\n",
    "    text3 = [tok for tok in text2 if tok.isalpha()] # retain only alphabetic words\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english')) # generate stopwords from English dictionary\n",
    "    text4 = [tok for tok in text3 if tok not in stop_words]\n",
    "    \n",
    "    # lemmatize tokens\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text5 = [wordnet_lemmatizer.lemmatize(tok) for tok in text4]\n",
    "    \n",
    "    # correct some misspellings\n",
    "    #text6 = [str(TextBlob(tok).correct()) for tok in text5]\n",
    "    \n",
    "    text_tokenized.append(text5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Create corpus w/ gensim\n",
    "\n",
    "We use [gensim](https://radimrehurek.com/gensim/) to create a corpus, where each token is mapped to a unique numerical ID and word count (i.e. bag of words, BoW) in order to set up structure for inputting to NLP algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\diana\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# create dictionary from list of pre-processed tokens (all instances) across all documents ('lemmatized')\n",
    "dictionary = Dictionary(text_tokenized)\n",
    "\n",
    "# generate corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in text_tokenized] # .doc2bow method converts documents into BoW format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a sample review under our different processing steps leading up to gensim corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review (after pre-processing):  i don't give 5 stars often, unless it was truly a stellar meal. and of course it doesn't have to be a stuffy fancypants place to be stellar, although sometimes it is. shuko was one of those upscale places and was truly, from the bottom of my heart, one of the best meals i've ever had. it's so badass that the door isn't even marked- it's this elite club that you enter because you know about it already. i got the omakase menu, every bite was carefully prepared in front of my eyes before being placed on my little stone tray and then popped into my mouth. it was as if centuries of preparation and thought had gone into the making of each bite; the culture and technique behind the assembly of the sushi, the flavor and texture profiles, the quality of the ingredients... i think when i went through that unmarked black door, i went to narnia and then reemerged into the mundane world upon exit. sake was phenomenal. i tasted everything that the sommelier (sakelier? sake master?) was describing. service was awesome and very pleasant, we had a blast with the bartender. if you can get a res and have the cash to finance, go to shuko. if not, picture how the gods eat and you got yourself a fair approximation of this meal \n",
      "\n",
      "Review (after document tokenization, removing stopwords, lemmatization):  ['give', 'star', 'often', 'unless', 'truly', 'stellar', 'meal', 'course', 'stuffy', 'fancypants', 'place', 'stellar', 'although', 'sometimes', 'shuko', 'one', 'upscale', 'place', 'truly', 'bottom', 'heart', 'one', 'best', 'meal', 'ever', 'badass', 'door', 'even', 'elite', 'club', 'enter', 'know', 'already', 'got', 'omakase', 'menu', 'every', 'bite', 'carefully', 'prepared', 'front', 'eye', 'placed', 'little', 'stone', 'tray', 'popped', 'mouth', 'century', 'preparation', 'thought', 'gone', 'making', 'bite', 'culture', 'technique', 'behind', 'assembly', 'sushi', 'flavor', 'texture', 'profile', 'quality', 'ingredient', 'think', 'went', 'unmarked', 'black', 'door', 'went', 'narnia', 'reemerged', 'mundane', 'world', 'upon', 'exit', 'sake', 'phenomenal', 'tasted', 'everything', 'sommelier', 'sakelier', 'sake', 'master', 'describing', 'service', 'awesome', 'pleasant', 'blast', 'bartender', 'get', 're', 'cash', 'finance', 'go', 'shuko', 'picture', 'god', 'eat', 'got', 'fair', 'approximation', 'meal'] \n",
      "\n",
      "Review (after gensim corpus):  [(34, 1), (40, 1), (44, 2), (48, 1), (60, 1), (85, 2), (108, 1), (123, 2), (130, 1), (157, 1), (184, 1), (191, 1), (193, 1), (194, 1), (201, 3), (210, 2), (229, 1), (267, 1), (282, 2), (287, 1), (300, 1), (347, 1), (363, 1), (374, 1), (570, 1), (606, 1), (613, 1), (652, 1), (657, 1), (679, 1), (720, 1), (737, 1), (752, 1), (784, 1), (848, 1), (867, 1), (949, 1), (978, 1), (1027, 1), (1063, 1), (1126, 2), (1132, 1), (1242, 1), (1275, 1), (1281, 2), (1341, 1), (1453, 1), (1518, 1), (1578, 1), (1604, 1), (1617, 1), (1633, 1), (1722, 1), (1731, 2), (1910, 1), (1920, 1), (1984, 1), (2013, 1), (2036, 1), (2077, 1), (2149, 1), (2211, 1), (2296, 1), (2318, 2), (2586, 1), (3029, 1), (3030, 1), (3200, 1), (3242, 1), (3290, 1), (3397, 1), (3839, 1), (3947, 1), (3984, 1), (4487, 1), (4974, 1), (5193, 1), (5225, 1), (5777, 1), (7175, 1), (7304, 1), (10177, 1), (14671, 1), (17834, 1), (23803, 1), (26091, 1), (29610, 2), (33976, 1), (39867, 1), (51851, 1), (51852, 1)]\n"
     ]
    }
   ],
   "source": [
    "print('Review (after pre-processing): ', text, '\\n')\n",
    "print('Review (after document tokenization, removing stopwords, lemmatization): ', text5, '\\n')\n",
    "print('Review (after gensim corpus): ', corpus[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframe of corpus, which tracks the restaurant that the review belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant_idx</th>\n",
       "      <th>corpus</th>\n",
       "      <th>yelp_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[(17, 1), (26, 1), (34, 1), (42, 1), (72, 1), ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[(14, 1), (29, 1), (30, 1), (42, 1), (44, 1), ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[(23, 1), (26, 1), (42, 1), (60, 2), (61, 1), ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[(4, 1), (13, 1), (29, 1), (42, 2), (50, 2), (...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   restaurant_idx                                             corpus  \\\n",
       "0               0  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n",
       "1               0  [(17, 1), (26, 1), (34, 1), (42, 1), (72, 1), ...   \n",
       "2               0  [(14, 1), (29, 1), (30, 1), (42, 1), (44, 1), ...   \n",
       "3               0  [(23, 1), (26, 1), (42, 1), (60, 2), (61, 1), ...   \n",
       "4               0  [(4, 1), (13, 1), (29, 1), (42, 2), (50, 2), (...   \n",
       "\n",
       "   yelp_rating  \n",
       "0            5  \n",
       "1            4  \n",
       "2            3  \n",
       "3            5  \n",
       "4            4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus = pd.DataFrame({'restaurant_idx':df_reviews['review_idx'], \n",
    "                          'corpus':corpus, \n",
    "                          'yelp_rating':df_reviews['rating']})\n",
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Basic word count and bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find most frequent words in best-rated and worst-rated Yelp restaurants\n",
    "\n",
    "- \"Good\" Yelp reviews have ratings = 5 \n",
    "- \"Bad\" Yelp reviews have ratings <= 3\n",
    "\n",
    "Note that individual reviews can only be an integer from 1 to 5. Overall average Yelp rating for a restaurant, however, is capable of increments of 0.5 (such as 4.5/5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best-rating reviews\n",
    "\n",
    "idx_good = df_reviews[df_reviews['rating'] == 5].index\n",
    "idx_good_doc = [t for t,j in df_corpus['restaurant_idx'].iteritems() if j in idx_good] # index of docs belonging to those restaurants\n",
    "\n",
    "subcorpus_good = []\n",
    "subcorpus_good = [(subcorpus_good + doc) for idx, doc in df_corpus.loc[idx_good_doc]['corpus'].iteritems()]\n",
    "\n",
    "# Worst-rating reviews\n",
    "\n",
    "idx_bad = df_reviews[df_reviews['rating'] <= 3 ].index\n",
    "idx_bad_doc = [t for t,j in df_corpus['restaurant_idx'].iteritems() if j in idx_bad] # index of docs belonging to those restaurants\n",
    "\n",
    "subcorpus_bad = []\n",
    "subcorpus_bad = [(subcorpus_bad + doc) for idx, doc in df_corpus.loc[idx_bad_doc]['corpus'].iteritems()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print top 10 words for \"good\" and \"bad\" Yelp reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for GOOD-rating Yelp reviews: \n",
      "\n",
      "food 34855\n",
      "good 27015\n",
      "place 24056\n",
      "dish 20569\n",
      "like 19130\n",
      "great 18700\n",
      "restaurant 18590\n",
      "one 18162\n",
      "service 16875\n",
      "would 16590\n",
      "\n",
      "\n",
      "Top 10 words for BAD-rating Yelp reviews: \n",
      "\n",
      "food 14010\n",
      "good 9392\n",
      "dish 8281\n",
      "place 8195\n",
      "restaurant 7776\n",
      "service 6912\n",
      "like 6870\n",
      "great 6812\n",
      "one 6387\n",
      "would 5782\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import itertools\n",
    "\n",
    "# Good-rating reviews\n",
    "\n",
    "total_word_count_good = collections.defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(subcorpus_good):\n",
    "    total_word_count_good[word_id] += word_count\n",
    "\n",
    "sorted_word_count_good = sorted(total_word_count_good.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "print('Top 10 words for GOOD-rating Yelp reviews:','\\n')\n",
    "for word_id, word_count in sorted_word_count_good[:10]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "print('\\n')\n",
    "\n",
    "# Bad-rating reviews\n",
    "\n",
    "total_word_count_bad = collections.defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(subcorpus_bad):\n",
    "    total_word_count_bad[word_id] += word_count\n",
    "\n",
    "sorted_word_count_bad = sorted(total_word_count_bad.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "print('Top 10 words for BAD-rating Yelp reviews:','\\n')\n",
    "for word_id, word_count in sorted_word_count_bad[:10]:\n",
    "    print(dictionary.get(word_id), word_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>: As seen from the overlap between top-10 \"good\"/\"bad\" words from a simple bag of words count, we will need more sophisticated tools to parse keywords associated with \"good\" or \"bad\" ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. tf-idf EDA\n",
    "\n",
    "In the previous section, we did simple pre-processing and simply took token frequency. Here, we experiment with using gensim's [tf-idf](https://radimrehurek.com/gensim/models/tfidfmodel.html) to identify most important words in each document. This is accomplished with their algorithm by down-weighting shared words (between documents) beyond simply stopwords, ensuring that common words don't show up as key words. Conversely, document-specific words are weighted highly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Experimenting with tf-idf on a Yelp review\n",
    "\n",
    "We generate tfidf weights for a single document (Yelp review) to see how tf-idf performs. The tf-idf model is generated on the entire corpus of documents (i.e. reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf weights:  \n",
      " [(0, 0.11605235285752101), (1, 0.055517904959346144), (2, 0.058550703583362056), (3, 0.01923654735257033), (4, 0.027415914582950895)] \n",
      "\n",
      "Top 5 weighted words:\n",
      "oden 0.43697383680554125\n",
      "dashi 0.2652405475558776\n",
      "uh 0.2273065414231621\n",
      "mochi 0.19016016546668416\n",
      "spaghetti 0.1868451957856557\n",
      "\n",
      "\n",
      "Text:  \n",
      " davelle uh oden uh foodie trippin get order right uh shawty look good eatin oden oden dish drink dashi davelle oden moonlight xxxtentacion rip everything amazing u dining tiny cozy cramped beautiful little spot got oden set karaage cod spaghetti hokkaido spaghetti uni tomato cold dish topped kinda optional light cheese drink dashi aaaalllll dish good soft blanched skinless savory daikon served spicy yuzu paste use sparingly pretty big kick red miso paste soft mushy perfectly cooked heart shaped daikon mochi lightly fried bag soft gooey delicious mochi def drink dashi scallion enoki mushroom ginger hanpen white fish cake soft texture airy typical fish cake denseness fishcake delicious served spicy yuzu paste sausage served japanese mustard yummy bamboo shoot cooked enough left slight hate mushy bamboo shoot dashi similar taste mochi karaage soft juicy chicken lightly battered medium crisp cod spaghetti aka mentaiko pasta light cod roe taste fishy delicious hokkaido style spaghetti uni delicious much uni guess maybe another variation mentaiko liquor license sake soju cocktail beer wine went tonight quiet small space sure peak time usually wait\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "doc = corpus[0]\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print('tfidf weights: ', '\\n', tfidf_weights[:5], '\\n')\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "print('Top 5 weighted words:')\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary[term_id], weight)\n",
    "\n",
    "print('\\n')\n",
    "print('Text: ','\\n', ' '.join(corpus_tokenized[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>: It appears that if we take a document to be a single review, tf-idf may pick keywords that are specific to the reviewed restaurant's cuisine. Although this may be useful for identifying what food the restaurant serves, we are more interested in what the reviewer thought of the food, service, etc. \n",
    "\n",
    "Next, we try taking a document to be the the concatenation of all reviews belonging to a single restaurant to see if we get more relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Experimenting with tf-idf on all reviews belonging to a single restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf weights:  \n",
      " [(0, 0.0385683994677437), (1, 0.018717773450899693), (2, 0.01961377610979211), (3, 0.006518710429663028), (4, 0.00919997238797367)] \n",
      "\n",
      "Top 5 weighted words:\n",
      "oden 0.17360491863449606\n",
      "oden 0.14467076552874672\n",
      "gobo 0.13137044585937177\n",
      "ada 0.10397982332595031\n",
      "dashi 0.0903979302813469\n"
     ]
    }
   ],
   "source": [
    "# Combine a single restaurant's reviews into one document (Davelle, first entry in df_nyt_yelp restaurant database)\n",
    "\n",
    "restaurant_idx = 0\n",
    "df = df_corpus[df_corpus['restaurant_idx']==0]\n",
    "doc = df['corpus'].tolist()\n",
    "doc = list(itertools.chain(*doc))\n",
    "\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print('tfidf weights: ', '\\n', tfidf_weights[:5], '\\n')\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "print('Top 5 weighted words:')\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary[term_id], weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion:</b> Top tf-idf keywords still seem to refer to the restaurant's type of cuisine more so than taste/food. It's likely that tf-idf, because it is designed to down-weight common words between documents, will actually leave out the phrases we want regarding food, service, and quality, since these are likely to appear across all documents (i.e. reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Summary\n",
    "\n",
    "We conclude this section with a pipeline setting up for converting documents (Yelp reviews) into token-wordcount mappings. As seen from the top wordcounts of \"best-rating\" and \"worst_rating\" Yelp reviews, there are many confounding terms that probably won't serve as good predictors for \"good\" or \"bad\" restaurants. \n",
    "\n",
    "Next, we'll experiment with word embeddings, sentiment analysis, CountVectorizer train-test-split on \"good\"/\"bad\" restaurants, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification: \"good\"/\"bad\" reviews\n",
    "\n",
    "Here, we use [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to train/predict review labels: \"good\" or \"bad\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Generating \"good\" & \"bad\" review labels\n",
    "\n",
    "Like the previous section, we'll take \"good\" ratings = 5 and \"bad\" ratings <= 3. As a result, we ignore reviews with a \"4\"-rating for now. \n",
    "\n",
    "The rationale is that these can contain a mix of positive/negative comments - negative comments explaining why the restaurant is not a 5, but also positive comments explaining why the restaurant would be > 3. Such a mix may confound our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate dataframe for classification train/test\n",
    "\n",
    "Generate dataframe we will be working with, which contains only reviews of <=3 & = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cool_count</th>\n",
       "      <th>elite_count</th>\n",
       "      <th>friend_count</th>\n",
       "      <th>funny_count</th>\n",
       "      <th>length_count</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_count</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_text</th>\n",
       "      <th>useful_count</th>\n",
       "      <th>user_count</th>\n",
       "      <th>review_idx</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>1771</td>\n",
       "      <td>5</td>\n",
       "      <td>345</td>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>Davelle, uh, oden, uh. Foodie, why you trippin...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jennie C.</td>\n",
       "      <td>0</td>\n",
       "      <td>davelle uh oden uh foodie trippin get order ri...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>486</td>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>2018-06-27</td>\n",
       "      <td>Lovely little 16 seater at the south end of th...</td>\n",
       "      <td>0</td>\n",
       "      <td>Adam W.</td>\n",
       "      <td>0</td>\n",
       "      <td>lovely little seater south end le went late lu...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>277</td>\n",
       "      <td>2</td>\n",
       "      <td>1583</td>\n",
       "      <td>5</td>\n",
       "      <td>96</td>\n",
       "      <td>2018-03-20</td>\n",
       "      <td>If you enjoy an small intimate cafe with diffe...</td>\n",
       "      <td>8</td>\n",
       "      <td>Maria S.</td>\n",
       "      <td>0</td>\n",
       "      <td>enjoy small intimate cafe different type japan...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cool_count  elite_count friend_count funny_count  length_count  rating  \\\n",
       "0          4            1           45           2          1771       5   \n",
       "3          1            1           55           0           486       5   \n",
       "6          2            1          277           2          1583       5   \n",
       "\n",
       "  review_count review_date                                        review_text  \\\n",
       "0          345  2018-06-18  Davelle, uh, oden, uh. Foodie, why you trippin...   \n",
       "3           58  2018-06-27  Lovely little 16 seater at the south end of th...   \n",
       "6           96  2018-03-20  If you enjoy an small intimate cafe with diffe...   \n",
       "\n",
       "  useful_count user_count  review_idx  \\\n",
       "0            4  Jennie C.           0   \n",
       "3            0    Adam W.           0   \n",
       "6            8   Maria S.           0   \n",
       "\n",
       "                                   text_preprocessed label  \n",
       "0  davelle uh oden uh foodie trippin get order ri...  good  \n",
       "3  lovely little seater south end le went late lu...  good  \n",
       "6  enjoy small intimate cafe different type japan...  good  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab \"good\" & \"bad\" review indices\n",
    "idx_class = idx_good.append(idx_bad)\n",
    "\n",
    "# Filter dataframe for only \"good\" and \"bad\" reviews. Save to 'df_class'\n",
    "df_class = df_reviews.loc[idx_class]\n",
    "\n",
    "# Include pre-processed text in new column: 'text_preprocessed'\n",
    "text_preprocessed = [' '.join(doc) for doc in text_tokenized]\n",
    "df = pd.DataFrame({'text':text_preprocessed})\n",
    "df_class['text_preprocessed'] = df.loc[idx_class]\n",
    "\n",
    "# Assign \"good\" or \"bad\" label\n",
    "df_class.loc[idx_good, 'label'] = 'good'\n",
    "df_class.loc[idx_bad, 'label'] = 'bad'\n",
    "\n",
    "df_class.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 CountVectorizer for train/test split\n",
    "\n",
    "Use CountVectorizer to convert text to a sparse <b>document-term matrix (DTM)</b> of token counts, where each column is a <b>token</b> from the corpus vocabulary (generated from training set), each row is a <b>document</b> (a Yelp review), and the values are token frequency. Train & fit to set up for next section of predicting \"good\"/\"bad\" reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aaa', 'aaaaaaaaaand', 'aaaaaaaand', 'aaaaaamazing', 'aaaaall', 'aaaalllll', 'aaaamazing', 'aaaand', 'aaaanyway', 'aaah', 'aaalll', 'aaamazing', 'aaanndd', 'aaawwweeesome', 'aahed', 'aahhhhh', 'aahing', 'aahs', 'aamer']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df_class.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_class['text_preprocessed'], y, test_size=.33, random_state = 53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Learn the \"vocabulary\" of the training set & transform into 'document-term' matrix\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "#  Use the fitted vocabulary to build a DTM from the testing data (IGNORES tokens it hasn't seen before)\n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiny sample of resulting sparse DTM, where rows = Yelp reviews, columns = vocabulary generated from training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DTM:  (34497, 37663)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amature</th>\n",
       "      <th>amazballs</th>\n",
       "      <th>amaze</th>\n",
       "      <th>amazeballs</th>\n",
       "      <th>amazed</th>\n",
       "      <th>amazeee</th>\n",
       "      <th>amazeeeeeeeballs</th>\n",
       "      <th>amazement</th>\n",
       "      <th>amazes</th>\n",
       "      <th>amazig</th>\n",
       "      <th>amazin</th>\n",
       "      <th>amazing</th>\n",
       "      <th>amazinggg</th>\n",
       "      <th>amazingggg</th>\n",
       "      <th>amazinggggg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amature  amazballs  amaze  amazeballs  amazed  amazeee  amazeeeeeeeballs  \\\n",
       "0        0          0      0           0       0        0                 0   \n",
       "\n",
       "   amazement  amazes  amazig  amazin  amazing  amazinggg  amazingggg  \\\n",
       "0          0       0       0       0        1          0           0   \n",
       "\n",
       "   amazinggggg  \n",
       "0            0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Shape of DTM: ', count_train.shape)\n",
    "pd.DataFrame(count_train[0,1000:1015].toarray(), columns=count_vectorizer.get_feature_names()[1000:1015])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's apparent that there will be many duplicate versions/misspellings of a word, which might confound results and cause some terms to be downweighted in importance. Not clear how to correct for these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Naive Bayes classifier\n",
    "\n",
    "The Naive Bayes model is commonly used for testing NLP classification problems. It is rooted in probability ([Bayes theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem)) and generates predictions based on past data - given prior training data with features and labelled outcomes, what can we predict with our set of test observations and their features? The label it predicts for each observation is based on its calculation of the likeliest out of the possible labels. See [here](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/) for a good explanation.\n",
    "\n",
    "Here, each word from `CountVectorizer` acts as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate predicted \"good\"/\"bad\" reviews\n",
    "\n",
    "Use sklearn's [naive_bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) module to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8936558380414312 \n",
      "\n",
      "Confusion matrix: \n",
      " [[9954  825]\n",
      " [ 982 5231]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print('Accuracy score:', score, '\\n')\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['good', 'bad'])\n",
    "print('Confusion matrix:','\\n', cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier performed fairly well. Let's inspect the model to actually see what it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad [(-13.98729597068333, 'aaaanyway'), (-13.98729597068333, 'aaah'), (-13.98729597068333, 'aaalll'), (-13.98729597068333, 'aahed'), (-13.98729597068333, 'aahing'), (-13.98729597068333, 'aamer'), (-13.98729597068333, 'aarp'), (-13.98729597068333, 'aaverage'), (-13.98729597068333, 'abaiyin'), (-13.98729597068333, 'abandoning'), (-13.98729597068333, 'abberation'), (-13.98729597068333, 'abbreviation'), (-13.98729597068333, 'aberation'), (-13.98729597068333, 'abercrombie'), (-13.98729597068333, 'abhorrent'), (-13.98729597068333, 'abiding'), (-13.98729597068333, 'abit'), (-13.98729597068333, 'abject'), (-13.98729597068333, 'abnormal'), (-13.98729597068333, 'abnoxious')] \n",
      "\n",
      "good [(-5.577465297595593, 'love'), (-5.55066228712551, 'come'), (-5.510299969018506, 'try'), (-5.4789417279342985, 'experience'), (-5.422646838110797, 'definitely'), (-5.403566255466849, 'meal'), (-5.180273014757853, 'amazing'), (-5.17803035851402, 'really'), (-5.150486088284966, 'best'), (-5.1146686047644945, 'menu'), (-5.071326659309726, 'time'), (-5.038320362841555, 'delicious'), (-5.014832149623423, 'like'), (-4.9822592318930266, 'service'), (-4.906950395323335, 'restaurant'), (-4.891132643769547, 'dish'), (-4.780060413660626, 'good'), (-4.712292085381426, 'great'), (-4.583446852522629, 'place'), (-4.257031491955898, 'food')]\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20], '\\n')\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results make sense. Negative words such as \"abberation\" and \"abominable\" feature prominently in \"bad\" reviews, while \"good\" reviews have positive descriptors such as \"delicious\" and \"amazing\". \n",
    "\n",
    "It is interesting to note features that by themselves are neutral, such as \"service\". Since they are included in \"good\" reviews, it seems service is an important factor and is conducted well in \"good\" reviews.\n",
    "\n",
    "However, it seems like reviewer misspellings could downweight the term's importance (ex. \"abnoxious\" vs \"obnoxious\"). There are a few duplicate words (ex. \"abberation\", \"aberation\") to the same effect as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check examples of false positives\n",
    "\n",
    "Where \"bad\" reviews were incorrectly classified as \"good\" reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 example:  \n",
      "\n",
      "Pre-processed:  great bistro ambiance bistro actual building gorgeous feel much like sitting nice bistro lyon nyc high end price view street delivery truck service fantastic gourgeres bar nice touch food good priced great ambiance crowded \n",
      "\n",
      "Actual review:  Great Bistro ambiance, at not so bistro prices.. . The actual building is gorgeous and feels very much like you are sitting in a nice bistro in Lyon, but with NYC high end prices, and views of 55th street delivery trucks and trashbags.. . The service is fantastic, and the gourgeres at the bar are a nice touch. The food is very good but over priced for what you get.. . Great ambiance when not crowded. \n",
      "\n",
      "First 10 examples: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37051    although definitely better pho chinatown nice ...\n",
       "47597    great bistro ambiance bistro actual building g...\n",
       "22326    white gold lunch game another level went back ...\n",
       "30444    bookmarked quite time picture seen instagram p...\n",
       "49345    let took trip ny specifically eat per se loved...\n",
       "4851     food pretty good flavor authentic tried nasi l...\n",
       "20765    good service place beautiful convinced food we...\n",
       "35277    would given star service faster gluten free pa...\n",
       "7839     try e alpukat coffee avocado milkshake one tim...\n",
       "60917    restaurant small cozy good spot date night sin...\n",
       "Name: text_preprocessed, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives = X_test[y_test < pred]\n",
    "print('1 example: ','\\n')\n",
    "print('Pre-processed: ', false_positives[47597], '\\n')\n",
    "print('Actual review: ', df_reviews.loc[47597, 'review_text'], '\\n')\n",
    "print('First 10 examples: ')\n",
    "false_positives.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, a likely reason some reviews were false positives was due to the prevalence of positive words (ex. \"good\", \"gorgeous\") in the midst of a negative evaluation with a few turns of phrase (ex. \"...but over priced for what you get\"). \n",
    "\n",
    "In fact, in our example, pre-processing and removing stopwords may have removed tokens that would have caused the review to be correctly labelled as \"bad\", since they were critical parts of negative turns of phrase. Phrases/words such as \"not so\", \"trashbags\", \"but over priced for what you get\" are lost as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check examples of false negatives\n",
    "\n",
    "Where \"good\" reviews were incorrectly classified as \"bad\" reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 example:  \n",
      "\n",
      "Pre-processed:  love place went first time ordered salt pepper chicken rice tofu vegetable rice beef tendon dish rice stayed meal complimentary pork soup tea nice soup tea like place mama lee made soup tea heart kindness know restarurant complimentary thing taste kinda bland shitty sometimes felt like home eating meal rice came separately another bowl even tho ordered rice worth \n",
      "\n",
      "Actual review:  Love this place. I went there for the first time and ordered salt and pepper chicken over rice, tofu with vegetable over rice,  and beef tendon dish over rice. We stayed for the meal and they have complimentary pork soup and tea. . . It's a very nice soup and tea, this is not like other places, Mama Lee made these soup and tea with her heart (kindness). You know how other restarurant complimentary things taste kinda just bland and shitty sometimes, this is not at all!   I felt like home eating all the meals. . . The rice came separately with another bowl even tho you ordered \"______over rice\"  . . It is worth it. \n",
      "\n",
      "First 10 examples: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14283    love place bummer ca since come visit nyc ever...\n",
       "18056    love place went first time ordered salt pepper...\n",
       "32238    brother stopped quick lunch food great loved a...\n",
       "60373    cute kitschy vibe decent price strong drink cu...\n",
       "54334    true service bad food take awhile come pizza g...\n",
       "46904    despite poke craze taking entire city place ju...\n",
       "60155    amazing place noodle tasty take low spice othe...\n",
       "14436    visited tim ho wan couple time must say go dim...\n",
       "13951    came tuesday evening line door already pretty ...\n",
       "7329     rice noodle mi fen decently made color texture...\n",
       "Name: text_preprocessed, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives = X_test[y_test > pred]\n",
    "\n",
    "print('1 example: ','\\n')\n",
    "print('Pre-processed: ', false_negatives[18056],'\\n')\n",
    "print('Actual review: ', df_reviews.loc[18056, 'review_text'], '\\n')\n",
    "print('First 10 examples: ')\n",
    "false_negatives.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the false positives, these reviews tend to have a mixed bag of vocabulary associated with both \"good\" & \"bad\" reviews. In the above example for instance, there are many positive words (ex. \"love\", \"nice\") but also negative words that would probably trigger a \"bad\" review (ex. \"shitty\", \"bland\") even though the reviewer used these terms to describe other competing restaurants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we: \n",
    "- Set up a pipeline for NLP - including pre-processing, generating a corpus, bag of words, CountVectorizer sparse DTM, etc. \n",
    "- Discovered words (and word frequencies) associated with \"good\" & \"bad\" reviews.\n",
    "- Successfully explored classification for \"good\" & \"bad\" reviews. Our Naive Bayes classifier worked fairly well, with an accuracy score of ~90%. \n",
    "\n",
    "Now that we have quantified terms associated with \"good\"/\"bad\" reviews, as well as experimented with a classifier for predicting such reviews, we proceed with incorporating NYT data in our next notebook (`data_EDA_timeseries`). \n",
    "\n",
    "The results in this section will allow us to determine whether the introduction of NYT reviews has any influence on Yelp reviews. For instance, introducing NYT reviews may shift the emphasis on different terms predicting \"good\"/\"bad\" reviews (ex. \"NYT\", \"service\", \"hole-in-the-wall\", \"critic\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
